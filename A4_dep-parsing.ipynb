{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39712 trees read.\n",
      "In dev.conll:\n",
      "1695 trees read.\n",
      "In test.conll:\n",
      "2408 trees read.\n"
     ]
    }
   ],
   "source": [
    "print('In train.conll:')\n",
    "with open('valid_data/valid_train.conll') as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f'{len(train_trees)} trees read.')\n",
    "\n",
    "print('In dev.conll:')\n",
    "with open('valid_data/valid_dev.conll') as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f'{len(dev_trees)} trees read.')\n",
    "\n",
    "print('In test.conll:')\n",
    "with open('valid_data/valid_test.conll') as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f'{len(test_trees)} trees read.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = '<ROOT_rel>'\n",
    "        self.word='<ROOT>'\n",
    "        self.pos='<ROOT_POS>'\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "class NullDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = -1\n",
    "        self.deprel = '<NULL_rel>'\n",
    "        self.word='<NULL>'\n",
    "        self.pos='<NULL_POS>'\n",
    "    def __repr__(self):\n",
    "        return \"<NULL>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        assert self.buffer\n",
    "        self.stack.append(self.buffer.pop())\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop(-2)\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop()\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46266\n"
     ]
    }
   ],
   "source": [
    "def build_vocab():\n",
    "    word_vocab = {}\n",
    "    pos_vocab = {}\n",
    "    rel_vocab={}\n",
    "    for trees in [train_trees, dev_trees, test_trees]:\n",
    "        for tree in trees:\n",
    "            # Assuming tokens is a list of words in the DependencyTree object\n",
    "            for deprel in tree.deprels.values():\n",
    "                word= deprel.word\n",
    "                pos= deprel.pos\n",
    "                rel= deprel.deprel\n",
    "                if word not in word_vocab:\n",
    "                    word_vocab[word] = len(word_vocab) + 1  # Assign a unique ID to each word\n",
    "                if pos not in pos_vocab:\n",
    "                    pos_vocab[pos] = len(pos_vocab) + 1  # Assign a unique ID to each word\n",
    "                if rel not in rel_vocab:\n",
    "                    rel_vocab[rel] = len(rel_vocab) + 1  # Assign a unique ID to each word\n",
    "        # Add special tokens <ROOT> and <NULL>\n",
    "    print(len(word_vocab))\n",
    "    word_vocab[\"<NULL>\"] = len(word_vocab) + 1\n",
    "    pos_vocab['<NULL_POS>']=len(pos_vocab) + 1\n",
    "    rel_vocab['<NULL_rel>']=len(rel_vocab) + 1\n",
    "    word_vocab['<ROOT>']=0\n",
    "    pos_vocab['<ROOT_POS>']=0\n",
    "    rel_vocab['<ROOT_rel>']=0\n",
    "    action_vocab={}\n",
    "    ### START YOUR CODE ###\n",
    "    for trees in [train_trees, dev_trees, test_trees]:\n",
    "        for deps in trees:\n",
    "            for deprel in deps.deprels.values():\n",
    "                if deprel.deprel !='root':\n",
    "                    key1='L<'+deprel.deprel+'>'\n",
    "                    key2='R<'+deprel.deprel+'>'\n",
    "                    if key1 not in action_vocab:\n",
    "                        action_vocab[key1]=len(action_vocab)+1\n",
    "                    if key2 not in action_vocab:\n",
    "                        action_vocab[key2]=len(action_vocab)+1\n",
    "    action_vocab['shift']=len(action_vocab)+1\n",
    "    action_vocab[\"R<root>\"]=0\n",
    "    return word_vocab,pos_vocab,rel_vocab ,action_vocab\n",
    "\n",
    "word_vocab, pos_vocab,rel_vocab,action_vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(state, deprels,word_vocab, pos_vocab,rel_vocab):\n",
    "    # Initialize feature vectors for words and POS tags\n",
    "    word_feature = []\n",
    "    pos_feature = []\n",
    "    rel_feature= []\n",
    "    # Get the top three words on stack and buffer\n",
    "    \n",
    "    stack_words_id = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "    buffer_words_id = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "\n",
    "    stack_word=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_word.append('<NULL>')\n",
    "        elif word_id==0:\n",
    "            stack_word.append('<ROOT>')\n",
    "        else:\n",
    "            stack_word.append(deprels[word_id].word)\n",
    "\n",
    "    buffer_word=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_word.append(\"<NULL>\")\n",
    "        elif word_id==0:\n",
    "            buffer_word.append(\"<ROOT>\")\n",
    "        else:\n",
    "            buffer_word.append(deprels[word_id].word)\n",
    "\n",
    "    stack_pos=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_pos.append(\"<NULL_POS>\")\n",
    "        elif word_id==0:\n",
    "            stack_pos.append(\"<ROOT_POS>\")\n",
    "        else:\n",
    "            stack_pos.append(deprels[word_id].pos)\n",
    "\n",
    "    buffer_pos=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_pos.append(\"<NULL_POS>\")\n",
    "        elif word_id==0:\n",
    "            buffer_pos.append(\"<ROOT_POS>\")\n",
    "        else:\n",
    "            buffer_pos.append(deprels[word_id].pos)\n",
    "\n",
    "    stack_rel=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_rel.append(\"<NULL_rel>\")\n",
    "        elif word_id==0:\n",
    "            stack_rel.append(\"<ROOT_rel>\")\n",
    "        else:\n",
    "            stack_rel.append(deprels[word_id].deprel)\n",
    "\n",
    "    buffer_rel=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_rel.append(\"<NULL_rel>\")\n",
    "        elif word_id==0:\n",
    "            buffer_rel.append(\"<ROOT_rel>\")\n",
    "        else:\n",
    "            buffer_rel.append(deprels[word_id].deprel)\n",
    "\n",
    "    \n",
    "    for word in stack_word:\n",
    "        word_feature.append(word_vocab[word])\n",
    "    for word in buffer_word:\n",
    "        word_feature.append(word_vocab[word])\n",
    "    for pos in stack_pos:\n",
    "        pos_feature.append(pos_vocab[pos])\n",
    "    for pos in buffer_pos:\n",
    "        pos_feature.append(pos_vocab[pos])\n",
    "    for rel in stack_rel:\n",
    "        rel_feature.append(rel_vocab[rel])\n",
    "    for rel in buffer_rel:\n",
    "        rel_feature.append(rel_vocab[rel])\n",
    "\n",
    "    return word_feature + pos_feature +rel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(dep_tree,word_vocab, pos_vocab,rel_vocab,action_vocab):\n",
    "    deprels = dep_tree.deprels\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    deprels_list=[]\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    transition_UAS=[]\n",
    "    transition_LAS=[]\n",
    "\n",
    "    for _, rel in deprels.items():\n",
    "        deprels_list.append((rel.head,rel.id))\n",
    "\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "        \n",
    "        # start your code here\n",
    "        feature= extract_features(state, deprels,word_vocab, pos_vocab,rel_vocab)\n",
    "        # label=action_vocab[\"R(\"+ stack_top1.deprel +\")\"]\n",
    "        features.append(feature)        \n",
    "        # end your code here\n",
    "                \n",
    "        if state.stack[-1] == 0:\n",
    "            state.shift()\n",
    "            key='shift'\n",
    "            index=action_vocab[key]\n",
    "            label=[0]*len(action_vocab)\n",
    "            label[index]=1\n",
    "            labels.append(label)\n",
    "            continue\n",
    "        \n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "        \n",
    "        if (stack_top1.id, stack_top2.id) in deprels_list:\n",
    "            deprels_list.remove((stack_top1.id, stack_top2.id))\n",
    "            state.left_arc(stack_top2.deprel)\n",
    "            childcount[stack_top1.id] -= 1\n",
    "            \n",
    "            key= 'L<'+stack_top2.deprel+'>'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "            transition_UAS.append([stack_top1.word,stack_top2.word,'left_arc'])\n",
    "            transition_LAS.append([stack_top1.word,stack_top2.word,'left_arc',stack_top2.deprel])\n",
    "\n",
    "        elif (stack_top2.id, stack_top1.id) in deprels_list and childcount[stack_top1.id] == 0:\n",
    "            deprels_list.remove((stack_top2.id, stack_top1.id))\n",
    "            state.right_arc(stack_top1.deprel)\n",
    "            childcount[stack_top2.id] -= 1\n",
    "\n",
    "            key='R<'+stack_top1.deprel +'>'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "            transition_UAS.append([stack_top2.word,stack_top1.word,'right_arc'])\n",
    "            transition_LAS.append([stack_top2.word,stack_top1.word,'right_arc',stack_top1.deprel])\n",
    "\n",
    "        else:\n",
    "            state.shift()\n",
    "            key='shift'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "        label=[0]*len(action_vocab)\n",
    "        label[index]=1\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "    return transition_LAS,transition_UAS,features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46266\n"
     ]
    }
   ],
   "source": [
    "word_vocab, pos_vocab,rel_vocab,action_vocab=build_vocab()\n",
    "def generate_train_data(trees):\n",
    "    features_list=[]\n",
    "    labels_list=[]\n",
    "    for i,tree in enumerate(trees):\n",
    "        _,_,features,labels=get_training_data(tree,word_vocab, pos_vocab,rel_vocab,action_vocab)\n",
    "        for feature in features:\n",
    "            features_list.append(feature)\n",
    "        for label in labels:\n",
    "            labels_list.append(label)\n",
    "    return features_list,labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_label=generate_train_data(train_trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "    def __init__(self, word_size,pos_size,input_size,emb_size, hidden_size, output_size):\n",
    "        super(Parser, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.emb_layer1 = nn.Embedding(word_size, emb_size)\n",
    "        self.emb_layer2 = nn.Embedding(pos_size, emb_size)\n",
    "        self.hidden_layer = nn.Linear(emb_size*input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, X): \n",
    "        emb_word_output = self.emb_layer1(X[:,int(0):int(self.input_size/2)])\n",
    "        emb_pos_output = self.emb_layer2(X[:,int(self.input_size/2):int(self.input_size)])\n",
    "        emb_output=torch.cat((emb_word_output,emb_pos_output),dim=1)\n",
    "        emb_output=emb_output.view(emb_output.size(0),-1)\n",
    "        hidden_output = F.relu(self.hidden_layer(emb_output))\n",
    "        output=self.output_layer(hidden_output)\n",
    "        # score=F.softmax(output,dim=-1)\n",
    "        return output\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "word_size=len(word_vocab)+1\n",
    "pos_size=len(pos_vocab)+1\n",
    "emb_size=50\n",
    "hidden_size=200\n",
    "output_size=len(action_vocab) # 78\n",
    "batch_size = 1024\n",
    "learning_rate = 0.001\n",
    "num_epochs=15\n",
    "input_size=12\n",
    "model=Parser(word_size,pos_size,input_size,emb_size, hidden_size,output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data, train_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legal_transition(action,stack,buffer):\n",
    "    if(action==-1):\n",
    "        if(len(stack)<=2):\n",
    "            return False\n",
    "    if(action==0):\n",
    "        if(len(buffer)<=0):\n",
    "            return False\n",
    "    if(action==1):\n",
    "        if(len(stack)<2):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(model, tree):\n",
    "    deprels = tree.deprels\n",
    "    words = list(reversed(deprels.values()))\n",
    "    \n",
    "    # poses=[word.pos for word in sentence]\n",
    "\n",
    "    # word_ids = [word_vocab[key] for key in words]\n",
    "    # pos_ids = [pos_vocab[key] for key in poses]\n",
    "    \n",
    "    stack = [RootDummy()]  # Initialize the stack with ROOT symbol\n",
    "    buffer = words # Initialize the buffer with input words\n",
    "    \n",
    "    transition_LAS = []  # Store transitions applied\n",
    "    transition_UAS = []  # Store transitions applied\n",
    "\n",
    "    stack_top3=[]\n",
    "    buffer_top3=[]\n",
    "    while len(buffer) > 0 or len(stack) > 1:\n",
    "        stack_top3 = stack[-3:] if len(stack) >= 3 else stack + [NullDummy()] * (3 - len(stack))\n",
    "        buffer_top3 = buffer[-3:] if len(buffer) >= 3 else buffer + [NullDummy()] * (3 - len(buffer))\n",
    "        \n",
    "        stack_word_id=[word_vocab[key.word] for key in stack_top3]\n",
    "        buffer_word_id=[word_vocab[key.word] for key in buffer_top3]    \n",
    "        stack_pos_id=[pos_vocab[key.pos] for key in stack_top3]\n",
    "        buffer_pos_id=[pos_vocab[key.pos] for key in buffer_top3]\n",
    "        stack_rel_id=[rel_vocab[key.deprel] for key in stack_top3]\n",
    "        buffer_rel_id=[rel_vocab[key.deprel] for key in buffer_top3]\n",
    "                \n",
    "        input_feature=stack_word_id+buffer_word_id+stack_pos_id+buffer_pos_id+stack_rel_id+buffer_rel_id\n",
    "        input_feature=torch.Tensor(input_feature).to(device).to(torch.long)\n",
    "        input_feature=torch.unsqueeze(input_feature, 0)\n",
    "        \n",
    "        output= model(input_feature)\n",
    "        output=F.softmax(output,dim=-1)[0]\n",
    "        has_legal=False\n",
    "        reversed_action_vocab = {value: key for key, value in action_vocab.items()}\n",
    "\n",
    "\n",
    "        for action in action_vocab: \n",
    "            if action[0]=='L':\n",
    "                if(is_legal_transition(-1,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "            if action[0]=='R':\n",
    "                if(is_legal_transition(1,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "            if action=='shift':\n",
    "                if(is_legal_transition(0,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "        if not has_legal:\n",
    "            break\n",
    "        index=np.argmax(np.array(output.tolist()))\n",
    "        action= reversed_action_vocab[index]\n",
    "        \n",
    "        action_direction=0\n",
    "        rel=None\n",
    "        if action[0]=='L':\n",
    "            action_direction=-1\n",
    "            rel=action[2:-1]\n",
    "        if action[0]=='R':\n",
    "            action_direction=1\n",
    "            rel=action[2:-1]\n",
    "\n",
    "        \n",
    "        if(action_direction==-1):\n",
    "            stack_top1=stack[-1]\n",
    "            stack_top2=stack[-2]\n",
    "            transition_LAS.append([stack_top1.word,stack_top2.word ,'left_arc',rel])\n",
    "            transition_UAS.append([stack_top1.word,stack_top2.word ,'left_arc'])\n",
    "            stack.pop(-2)\n",
    "        elif(action_direction==0):\n",
    "            stack.append(buffer.pop())\n",
    "        elif(action_direction==1):\n",
    "            stack_top1=stack[-1]\n",
    "            stack_top2=stack[-2]\n",
    "            transition_LAS.append([stack_top2.word,stack_top1.word ,\"right_arc\",rel])\n",
    "            transition_UAS.append([stack_top2.word,stack_top1.word ,\"right_arc\"])\n",
    "            stack.pop()     \n",
    "    return transition_LAS,transition_UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_transition,true_transition):\n",
    "    # print(predict_transition)\n",
    "    # print(true_transition)\n",
    "    predict_transition_tuples = [tuple(item) for item in predict_transition]\n",
    "    true_transition_tuples = [tuple(item) for item in true_transition]\n",
    "    set_predict_transition = set(predict_transition_tuples)\n",
    "    set_true_transition = set(true_transition_tuples)\n",
    "\n",
    "    # 统计集合的交集大小\n",
    "    right = len(set_predict_transition.intersection(set_true_transition))\n",
    "    return right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch}/{num_epochs}', unit='batch') as pbar:\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data=data.to(device).to(torch.long)\n",
    "                target=target.to(device).to(torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': total_loss / len(train_loader)})\n",
    "                pbar.update(1)\n",
    "        if(epoch%3==0):\n",
    "            model.eval()\n",
    "            best_score=0\n",
    "            total_score=0\n",
    "            LAS_score=0\n",
    "            UAS_score=0\n",
    "            for dev_tree in dev_trees:\n",
    "                predict_transition_LAS,predict_transition_UAS= parse_sentence(model,dev_tree)\n",
    "                true_transition_LAS,true_transition_UAS,_,_=get_training_data(dev_tree,word_vocab,pos_vocab,rel_vocab,action_vocab)\n",
    "                UAS_score+=evaluate(predict_transition_UAS,true_transition_UAS)\n",
    "                LAS_score+=evaluate(predict_transition_LAS,true_transition_LAS)\n",
    "                total_score+=len(true_transition_LAS)\n",
    "            dev_LAS_score= LAS_score/total_score*100\n",
    "            dev_UAS_score= UAS_score/total_score*100\n",
    "            print('dev_LAS_score: ', dev_LAS_score,\"%\")\n",
    "            print('dev_UAS_score: ', dev_UAS_score,\"%\")\n",
    "            if dev_LAS_score>best_score:\n",
    "                best_score=dev_LAS_score\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 1848/1848 [00:20<00:00, 90.69batch/s, Loss=0.491] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.49084597796169455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 1848/1848 [00:22<00:00, 81.29batch/s, Loss=0.349] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.34906297578633605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 1848/1848 [00:21<00:00, 84.51batch/s, Loss=0.315] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  71.82432432432432 %\n",
      "dev_UAS_score:  77.55755755755756 %\n",
      "Epoch 3/15, Loss: 0.31476702820912844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 1848/1848 [00:19<00:00, 95.88batch/s, Loss=0.291] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.2911701915035774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 1848/1848 [00:22<00:00, 82.50batch/s, Loss=0.273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.2727889334460422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 1848/1848 [00:21<00:00, 85.06batch/s, Loss=0.257] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  72.93793793793793 %\n",
      "dev_UAS_score:  78.64614614614615 %\n",
      "Epoch 6/15, Loss: 0.2573623631181784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 1848/1848 [00:22<00:00, 82.99batch/s, Loss=0.244] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.2439927805207057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 1848/1848 [00:22<00:00, 80.97batch/s, Loss=0.232] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Loss: 0.23199493312619593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 1848/1848 [00:19<00:00, 96.83batch/s, Loss=0.221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  73.01301301301302 %\n",
      "dev_UAS_score:  78.74874874874875 %\n",
      "Epoch 9/15, Loss: 0.22099000648534917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 1848/1848 [00:22<00:00, 83.98batch/s, Loss=0.211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Loss: 0.21083346293086097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 1848/1848 [00:21<00:00, 85.23batch/s, Loss=0.201] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Loss: 0.20129444397324736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 1848/1848 [00:18<00:00, 98.17batch/s, Loss=0.192] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  72.505005005005 %\n",
      "dev_UAS_score:  78.32082082082083 %\n",
      "Epoch 12/15, Loss: 0.19225367509006036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 1848/1848 [00:22<00:00, 80.57batch/s, Loss=0.184]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Loss: 0.18369646536516937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 1848/1848 [00:21<00:00, 87.56batch/s, Loss=0.176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Loss: 0.1755066665345037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 1848/1848 [00:19<00:00, 93.29batch/s, Loss=0.168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  71.87187187187187 %\n",
      "dev_UAS_score:  77.75275275275276 %\n",
      "Epoch 15/15, Loss: 0.16772540326500351\n"
     ]
    }
   ],
   "source": [
    "train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_LAS_score:  72.89201445475803 %\n",
      "test_UAS_score:  78.65088925104513 %\n"
     ]
    }
   ],
   "source": [
    "model=Parser(word_size,pos_size,input_size,emb_size, hidden_size,output_size).to(device)\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "total_score=0\n",
    "LAS_score=0\n",
    "UAS_score=0\n",
    "for test_tree in test_trees:\n",
    "    predict_transition_LAS,predict_transition_UAS= parse_sentence(model,test_tree)\n",
    "    true_transition_LAS,true_transition_UAS,_,_=get_training_data(test_tree,word_vocab,pos_vocab,rel_vocab,action_vocab)\n",
    "    UAS_score+=evaluate(predict_transition_UAS,true_transition_UAS)\n",
    "    LAS_score+=evaluate(predict_transition_LAS,true_transition_LAS)\n",
    "    total_score+=len(true_transition_LAS)\n",
    "\n",
    "test_LAS_score= LAS_score/total_score*100\n",
    "test_UAS_score= UAS_score/total_score*100\n",
    "print('test_LAS_score: ', test_LAS_score,\"%\")\n",
    "print('test_UAS_score: ', test_UAS_score,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
