{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39712 trees read.\n",
      "In dev.conll:\n",
      "1695 trees read.\n",
      "In test.conll:\n",
      "2408 trees read.\n"
     ]
    }
   ],
   "source": [
    "print('In train.conll:')\n",
    "with open('valid_data/valid_train.conll') as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f'{len(train_trees)} trees read.')\n",
    "\n",
    "print('In dev.conll:')\n",
    "with open('valid_data/valid_dev.conll') as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f'{len(dev_trees)} trees read.')\n",
    "\n",
    "print('In test.conll:')\n",
    "with open('valid_data/valid_test.conll') as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f'{len(test_trees)} trees read.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = '<ROOT_rel>'\n",
    "        self.word='<ROOT>'\n",
    "        self.pos='<ROOT_POS>'\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "class NullDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = -1\n",
    "        self.deprel = '<NULL_rel>'\n",
    "        self.word='<NULL>'\n",
    "        self.pos='<NULL_POS>'\n",
    "    def __repr__(self):\n",
    "        return \"<NULL>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        assert self.buffer\n",
    "        self.stack.append(self.buffer.pop())\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop(-2)\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop()\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46266\n"
     ]
    }
   ],
   "source": [
    "def build_vocab():\n",
    "    word_vocab = {}\n",
    "    pos_vocab = {}\n",
    "    rel_vocab={}\n",
    "    for trees in [train_trees, dev_trees, test_trees]:\n",
    "        for tree in trees:\n",
    "            # Assuming tokens is a list of words in the DependencyTree object\n",
    "            for deprel in tree.deprels.values():\n",
    "                word= deprel.word\n",
    "                pos= deprel.pos\n",
    "                rel= deprel.deprel\n",
    "                if word not in word_vocab:\n",
    "                    word_vocab[word] = len(word_vocab) + 1  # Assign a unique ID to each word\n",
    "                if pos not in pos_vocab:\n",
    "                    pos_vocab[pos] = len(pos_vocab) + 1  # Assign a unique ID to each word\n",
    "                if rel not in rel_vocab:\n",
    "                    rel_vocab[rel] = len(rel_vocab) + 1  # Assign a unique ID to each word\n",
    "        # Add special tokens <ROOT> and <NULL>\n",
    "    print(len(word_vocab))\n",
    "    word_vocab[\"<NULL>\"] = len(word_vocab) + 1\n",
    "    pos_vocab['<NULL_POS>']=len(pos_vocab) + 1\n",
    "    rel_vocab['<NULL_rel>']=len(rel_vocab) + 1\n",
    "    word_vocab['<ROOT>']=0\n",
    "    pos_vocab['<ROOT_POS>']=0\n",
    "    rel_vocab['<ROOT_rel>']=0\n",
    "    action_vocab={}\n",
    "    ### START YOUR CODE ###\n",
    "    for trees in [train_trees, dev_trees, test_trees]:\n",
    "        for deps in trees:\n",
    "            for deprel in deps.deprels.values():\n",
    "                if deprel.deprel !='root':\n",
    "                    key1='L<'+deprel.deprel+'>'\n",
    "                    key2='R<'+deprel.deprel+'>'\n",
    "                    if key1 not in action_vocab:\n",
    "                        action_vocab[key1]=len(action_vocab)+1\n",
    "                    if key2 not in action_vocab:\n",
    "                        action_vocab[key2]=len(action_vocab)+1\n",
    "    action_vocab['shift']=len(action_vocab)+1\n",
    "    action_vocab[\"R<root>\"]=0\n",
    "    return word_vocab,pos_vocab,rel_vocab ,action_vocab\n",
    "\n",
    "word_vocab, pos_vocab,rel_vocab,action_vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(state, deprels,word_vocab, pos_vocab,rel_vocab):\n",
    "    # Initialize feature vectors for words and POS tags\n",
    "    word_feature = []\n",
    "    pos_feature = []\n",
    "    rel_feature= []\n",
    "    # Get the top three words on stack and buffer\n",
    "    \n",
    "    stack_words_id = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "    buffer_words_id = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "\n",
    "    stack_word=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_word.append('<NULL>')\n",
    "        elif word_id==0:\n",
    "            stack_word.append('<ROOT>')\n",
    "        else:\n",
    "            stack_word.append(deprels[word_id].word)\n",
    "\n",
    "    buffer_word=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_word.append(\"<NULL>\")\n",
    "        elif word_id==0:\n",
    "            buffer_word.append(\"<ROOT>\")\n",
    "        else:\n",
    "            buffer_word.append(deprels[word_id].word)\n",
    "\n",
    "    stack_pos=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_pos.append(\"<NULL_POS>\")\n",
    "        elif word_id==0:\n",
    "            stack_pos.append(\"<ROOT_POS>\")\n",
    "        else:\n",
    "            stack_pos.append(deprels[word_id].pos)\n",
    "\n",
    "    buffer_pos=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_pos.append(\"<NULL_POS>\")\n",
    "        elif word_id==0:\n",
    "            buffer_pos.append(\"<ROOT_POS>\")\n",
    "        else:\n",
    "            buffer_pos.append(deprels[word_id].pos)\n",
    "\n",
    "    stack_rel=[]\n",
    "    for word_id in stack_words_id:\n",
    "        if word_id==-1:\n",
    "            stack_rel.append(\"<NULL_rel>\")\n",
    "        elif word_id==0:\n",
    "            stack_rel.append(\"<ROOT_rel>\")\n",
    "        else:\n",
    "            stack_rel.append(deprels[word_id].deprel)\n",
    "\n",
    "    buffer_rel=[]\n",
    "    for word_id in buffer_words_id:\n",
    "        if word_id==-1:\n",
    "            buffer_rel.append(\"<NULL_rel>\")\n",
    "        elif word_id==0:\n",
    "            buffer_rel.append(\"<ROOT_rel>\")\n",
    "        else:\n",
    "            buffer_rel.append(deprels[word_id].deprel)\n",
    "\n",
    "    \n",
    "    for word in stack_word:\n",
    "        word_feature.append(word_vocab[word])\n",
    "    for word in buffer_word:\n",
    "        word_feature.append(word_vocab[word])\n",
    "    for pos in stack_pos:\n",
    "        pos_feature.append(pos_vocab[pos])\n",
    "    for pos in buffer_pos:\n",
    "        pos_feature.append(pos_vocab[pos])\n",
    "    for rel in stack_rel:\n",
    "        rel_feature.append(rel_vocab[rel])\n",
    "    for rel in buffer_rel:\n",
    "        rel_feature.append(rel_vocab[rel])\n",
    "\n",
    "    return word_feature + pos_feature +rel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(dep_tree,word_vocab, pos_vocab,rel_vocab,action_vocab):\n",
    "    deprels = dep_tree.deprels\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    deprels_list=[]\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    transition_UAS=[]\n",
    "    transition_LAS=[]\n",
    "\n",
    "    for _, rel in deprels.items():\n",
    "        deprels_list.append((rel.head,rel.id))\n",
    "\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "        \n",
    "        # start your code here\n",
    "        feature= extract_features(state, deprels,word_vocab, pos_vocab,rel_vocab)\n",
    "        # label=action_vocab[\"R(\"+ stack_top1.deprel +\")\"]\n",
    "        features.append(feature)        \n",
    "        # end your code here\n",
    "                \n",
    "        if state.stack[-1] == 0:\n",
    "            state.shift()\n",
    "            key='shift'\n",
    "            index=action_vocab[key]\n",
    "            label=[0]*len(action_vocab)\n",
    "            label[index]=1\n",
    "            labels.append(label)\n",
    "            continue\n",
    "        \n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "        \n",
    "        if (stack_top1.id, stack_top2.id) in deprels_list:\n",
    "            deprels_list.remove((stack_top1.id, stack_top2.id))\n",
    "            state.left_arc(stack_top2.deprel)\n",
    "            childcount[stack_top1.id] -= 1\n",
    "            \n",
    "            key= 'L<'+stack_top2.deprel+'>'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "            transition_UAS.append([stack_top1.word,stack_top2.word,'left_arc'])\n",
    "            transition_LAS.append([stack_top1.word,stack_top2.word,'left_arc',stack_top2.deprel])\n",
    "\n",
    "        elif (stack_top2.id, stack_top1.id) in deprels_list and childcount[stack_top1.id] == 0:\n",
    "            deprels_list.remove((stack_top2.id, stack_top1.id))\n",
    "            state.right_arc(stack_top1.deprel)\n",
    "            childcount[stack_top2.id] -= 1\n",
    "\n",
    "            key='R<'+stack_top1.deprel +'>'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "            transition_UAS.append([stack_top2.word,stack_top1.word,'right_arc'])\n",
    "            transition_LAS.append([stack_top2.word,stack_top1.word,'right_arc',stack_top1.deprel])\n",
    "\n",
    "        else:\n",
    "            state.shift()\n",
    "            key='shift'\n",
    "            index=action_vocab[key]\n",
    "\n",
    "        label=[0]*len(action_vocab)\n",
    "        label[index]=1\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "    return transition_LAS,transition_UAS,features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46266\n"
     ]
    }
   ],
   "source": [
    "word_vocab, pos_vocab,rel_vocab,action_vocab=build_vocab()\n",
    "def generate_train_data(trees):\n",
    "    features_list=[]\n",
    "    labels_list=[]\n",
    "    for i,tree in enumerate(trees):\n",
    "        _,_,features,labels=get_training_data(tree,word_vocab, pos_vocab,rel_vocab,action_vocab)\n",
    "        for feature in features:\n",
    "            features_list.append(feature)\n",
    "        for label in labels:\n",
    "            labels_list.append(label)\n",
    "    return features_list,labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_label=generate_train_data(train_trees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "    def __init__(self, word_size,pos_size,rel_size,input_size,emb_size, hidden_size, output_size):\n",
    "        super(Parser, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.emb_layer1 = nn.Embedding(word_size, emb_size)\n",
    "        self.emb_layer2 = nn.Embedding(pos_size, emb_size)\n",
    "        self.emb_layer3 = nn.Embedding(rel_size, emb_size) \n",
    "        self.hidden_layer = nn.Linear(emb_size*input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, X): \n",
    "        emb_word_output = self.emb_layer1(X[:,int(0):int(self.input_size/3)])\n",
    "        emb_pos_output = self.emb_layer2(X[:,int(self.input_size/3):int(self.input_size*2/3)])\n",
    "        emb_rel_output = self.emb_layer3(X[:,int(self.input_size*2/3):int(self.input_size)])\n",
    "        emb_output=torch.cat((emb_word_output,emb_pos_output,emb_rel_output),dim=1)\n",
    "        emb_output=emb_output.view(emb_output.size(0),-1)\n",
    "        hidden_output = F.relu(self.hidden_layer(emb_output))\n",
    "        output=self.output_layer(hidden_output)\n",
    "        # score=F.softmax(output,dim=-1)  后边交叉熵损失集成了，这里就不用计算了\n",
    "        return output\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "word_size=len(word_vocab)\n",
    "pos_size=len(pos_vocab)\n",
    "rel_size=len(rel_vocab)\n",
    "emb_size=50\n",
    "hidden_size=200\n",
    "output_size=len(action_vocab) # 78\n",
    "batch_size = 1024\n",
    "learning_rate = 0.001\n",
    "num_epochs=6\n",
    "input_size=18\n",
    "model=Parser(word_size,pos_size,rel_size,input_size,emb_size, hidden_size,output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data, train_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理的时候要检查\n",
    "def is_legal_transition(action,stack,buffer):\n",
    "    if(action==-1):\n",
    "        if(len(stack)<=2):\n",
    "            return False\n",
    "    if(action==0):\n",
    "        if(len(buffer)<=0):\n",
    "            return False\n",
    "    if(action==1):\n",
    "        if(len(stack)<2):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(model, tree):\n",
    "    deprels = tree.deprels\n",
    "    words = list(reversed(deprels.values()))\n",
    "    \n",
    "    # poses=[word.pos for word in sentence]\n",
    "\n",
    "    # word_ids = [word_vocab[key] for key in words]\n",
    "    # pos_ids = [pos_vocab[key] for key in poses]\n",
    "    \n",
    "    stack = [RootDummy()]  # Initialize the stack with ROOT symbol\n",
    "    buffer = words # Initialize the buffer with input words\n",
    "    \n",
    "    transition_LAS = []  # Store transitions applied\n",
    "    transition_UAS = []  # Store transitions applied\n",
    "\n",
    "    stack_top3=[]\n",
    "    buffer_top3=[]\n",
    "    while len(buffer) > 0 or len(stack) > 1:\n",
    "        stack_top3 = stack[-3:] if len(stack) >= 3 else stack + [NullDummy()] * (3 - len(stack))\n",
    "        buffer_top3 = buffer[-3:] if len(buffer) >= 3 else buffer + [NullDummy()] * (3 - len(buffer))\n",
    "        \n",
    "        stack_word_id=[word_vocab[key.word] for key in stack_top3]\n",
    "        buffer_word_id=[word_vocab[key.word] for key in buffer_top3]    \n",
    "        stack_pos_id=[pos_vocab[key.pos] for key in stack_top3]\n",
    "        buffer_pos_id=[pos_vocab[key.pos] for key in buffer_top3]\n",
    "        stack_rel_id=[rel_vocab[key.deprel] for key in stack_top3]\n",
    "        buffer_rel_id=[rel_vocab[key.deprel] for key in buffer_top3]\n",
    "                \n",
    "        input_feature=stack_word_id+buffer_word_id+stack_pos_id+buffer_pos_id+stack_rel_id+buffer_rel_id\n",
    "        input_feature=torch.Tensor(input_feature).to(device).to(torch.long)\n",
    "        input_feature=torch.unsqueeze(input_feature, 0)\n",
    "        \n",
    "        output= model(input_feature)\n",
    "        output=F.softmax(output,dim=-1)[0]\n",
    "        has_legal=False\n",
    "        reversed_action_vocab = {value: key for key, value in action_vocab.items()}\n",
    "\n",
    "\n",
    "        for action in action_vocab: \n",
    "            if action[0]=='L':\n",
    "                if(is_legal_transition(-1,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "            if action[0]=='R':\n",
    "                if(is_legal_transition(1,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "            if action=='shift':\n",
    "                if(is_legal_transition(0,stack,buffer)):\n",
    "                    has_legal=True\n",
    "                else:\n",
    "                    output[action_vocab[action]]=-1\n",
    "\n",
    "        if not has_legal:\n",
    "            break\n",
    "        index=np.argmax(np.array(output.tolist()))\n",
    "        action= reversed_action_vocab[index]\n",
    "        \n",
    "        action_direction=0\n",
    "        rel=None\n",
    "        if action[0]=='L':\n",
    "            action_direction=-1\n",
    "            rel=action[2:-1]\n",
    "        if action[0]=='R':\n",
    "            action_direction=1\n",
    "            rel=action[2:-1]\n",
    "\n",
    "        \n",
    "        if(action_direction==-1):\n",
    "            stack_top1=stack[-1]\n",
    "            stack_top2=stack[-2]\n",
    "            transition_LAS.append([stack_top1.word,stack_top2.word ,'left_arc',rel])\n",
    "            transition_UAS.append([stack_top1.word,stack_top2.word ,'left_arc'])\n",
    "            stack.pop(-2)\n",
    "        elif(action_direction==0):\n",
    "            stack.append(buffer.pop())\n",
    "        elif(action_direction==1):\n",
    "            stack_top1=stack[-1]\n",
    "            stack_top2=stack[-2]\n",
    "            transition_LAS.append([stack_top2.word,stack_top1.word ,\"right_arc\",rel])\n",
    "            transition_UAS.append([stack_top2.word,stack_top1.word ,\"right_arc\"])\n",
    "            stack.pop()     \n",
    "    return transition_LAS,transition_UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_transition,true_transition):\n",
    "    # print(predict_transition)\n",
    "    # print(true_transition)\n",
    "    predict_transition_tuples = [tuple(item) for item in predict_transition]\n",
    "    true_transition_tuples = [tuple(item) for item in true_transition]\n",
    "    set_predict_transition = set(predict_transition_tuples)\n",
    "    set_true_transition = set(true_transition_tuples)\n",
    "\n",
    "    # 统计集合的交集大小\n",
    "    right = len(set_predict_transition.intersection(set_true_transition))\n",
    "    return right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch}/{num_epochs}', unit='batch') as pbar:\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data=data.to(device).to(torch.long)\n",
    "                target=target.to(device).to(torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': total_loss / len(train_loader)})\n",
    "                pbar.update(1)\n",
    "        if(epoch%3==0):\n",
    "            model.eval()\n",
    "            best_score=0\n",
    "            total_score=0\n",
    "            LAS_score=0\n",
    "            UAS_score=0\n",
    "            for dev_tree in dev_trees:\n",
    "                predict_transition_LAS,predict_transition_UAS= parse_sentence(model,dev_tree)\n",
    "                true_transition_LAS,true_transition_UAS,_,_=get_training_data(dev_tree,word_vocab,pos_vocab,rel_vocab,action_vocab)\n",
    "                UAS_score+=evaluate(predict_transition_UAS,true_transition_UAS)\n",
    "                LAS_score+=evaluate(predict_transition_LAS,true_transition_LAS)\n",
    "                total_score+=len(true_transition_LAS)\n",
    "            dev_LAS_score= LAS_score/total_score*100\n",
    "            dev_UAS_score= UAS_score/total_score*100\n",
    "            print('dev_LAS_score: ', dev_LAS_score,\"%\")\n",
    "            print('dev_UAS_score: ', dev_UAS_score,\"%\")\n",
    "            if dev_LAS_score>best_score:\n",
    "                best_score=dev_LAS_score\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|██████████| 1848/1848 [00:14<00:00, 128.44batch/s, Loss=0.133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9, Loss: 0.13299296245390138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/9: 100%|██████████| 1848/1848 [00:13<00:00, 139.55batch/s, Loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/9, Loss: 0.06313275216647234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/9: 100%|██████████| 1848/1848 [00:14<00:00, 127.74batch/s, Loss=0.0528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  91.6941941941942 %\n",
      "dev_UAS_score:  91.66916916916918 %\n",
      "Epoch 3/9, Loss: 0.05284616362214798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/9: 100%|██████████| 1848/1848 [00:13<00:00, 138.73batch/s, Loss=0.0452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/9, Loss: 0.04520147652274554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/9: 100%|██████████| 1848/1848 [00:14<00:00, 127.88batch/s, Loss=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/9, Loss: 0.03864230540403653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/9: 100%|██████████| 1848/1848 [00:14<00:00, 128.22batch/s, Loss=0.0328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  91.64664664664664 %\n",
      "dev_UAS_score:  91.62412412412412 %\n",
      "Epoch 6/9, Loss: 0.0327879844457153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/9: 100%|██████████| 1848/1848 [00:13<00:00, 139.44batch/s, Loss=0.0277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/9, Loss: 0.027730163716453076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/9: 100%|██████████| 1848/1848 [00:14<00:00, 127.27batch/s, Loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/9, Loss: 0.02320549653861188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/9: 100%|██████████| 1848/1848 [00:14<00:00, 127.53batch/s, Loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_LAS_score:  91.30630630630631 %\n",
      "dev_UAS_score:  91.29129129129129 %\n",
      "Epoch 9/9, Loss: 0.0194670050054712\n"
     ]
    }
   ],
   "source": [
    "train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_LAS_score:  92.09062566428116 %\n",
      "test_UAS_score:  92.09948274640402 %\n"
     ]
    }
   ],
   "source": [
    "model=Parser(word_size,pos_size,rel_size,input_size,emb_size, hidden_size,output_size).to(device)\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "total_score=0\n",
    "LAS_score=0\n",
    "UAS_score=0\n",
    "for test_tree in test_trees:\n",
    "    predict_transition_LAS,predict_transition_UAS= parse_sentence(model,test_tree)\n",
    "    true_transition_LAS,true_transition_UAS,_,_=get_training_data(test_tree,word_vocab,pos_vocab,rel_vocab,action_vocab)\n",
    "    UAS_score+=evaluate(predict_transition_UAS,true_transition_UAS)\n",
    "    LAS_score+=evaluate(predict_transition_LAS,true_transition_LAS)\n",
    "    total_score+=len(true_transition_LAS)\n",
    "\n",
    "test_LAS_score= LAS_score/total_score*100\n",
    "test_UAS_score= UAS_score/total_score*100\n",
    "print('test_LAS_score: ', test_LAS_score,\"%\")\n",
    "print('test_UAS_score: ', test_UAS_score,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
